# link

- demos를 활용한 그래프 이해 2d(https://www.desmos.com/calculator)
  - [sigmoid함수_demos로 만들어봄](https://www.desmos.com/calculator/zdoveye7qb)
  - [sine함수_demos로 만들어봄](https://www.desmos.com/calculator/3vdwmq0k54)
  - [cosine함수_demos로 만들어봄](https://www.desmos.com/calculator/3h05h6npn6)
  - [tangent함수_demos로 만들어봄](https://www.desmos.com/calculator/ebjjy9kzhm)

- 3D GeoGebra를 이용한 3d 매트릭스 이해(https://www.geogebra.org/3d?lang=en)
  - [3D Matrix Transformations | shaunteaches유튜브 tutorial영상](https://youtu.be/9L0Edn4bkY4?si=3XDzKSsDDzIFR5Zz)

- 수학용어 역사
  - [(수학용어)사인·코사인·탄젠트 sin cos tan](https://lovejk7000.tistory.com/279)

<hr />

- [수학기초 영어로 잘 정리됨github](https://github.com/ossu/math)
- [LaTex문법 잘정리됨. 공식 넣을때 굿](https://en.wikibooks.org/wiki/LaTeX/Mathematics)
  - [위키백과에 Tex문법 정리 잘됨](https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:TeX_%EB%AC%B8%EB%B2%95)
  - [깃허브 공식 문서 잘 정리됨.](https://docs.github.com/ko/enterprise-cloud@latest/get-started/writing-on-github/working-with-advanced-formatting/writing-mathematical-expressions)
  - [https://en.wikibooks.org/wiki/LaTeX/Mathematics](https://en.wikibooks.org/wiki/LaTeX/Mathematics)

- [C언어 매트릭스 이해하기](https://github.com/YoungHaKim7/Algorithm_Training/tree/main/05_C_CPP_2D_3D_Algorithm)

- 행렬matrix기초지식
  - [행렬 기본 연산 | 선형대수학의 행렬방법, 파트 1 | MATLAB Korea](#행렬-기본-연산--선형대수학의-행렬방법-파트-1--matlab-korea)

- Vector벡터 그림으로 이해
  - [Why is the determinant like that? | broke math student](https://youtu.be/Sv7VseMsOQc?si=kooGvwD3DKEPokGB) 
  - [그림으로 벡터부터 원리를 잘 보여줌 굿ALL of calculus 3 in 8 minutes. | gregorian calendar](https://youtu.be/5kwz7ajxfyA?si=sIcX0AWHznfGkYlU)
    - 0:17 3D Space, Vectors, and Surfaces
    - 0:44 Vector Multiplication
    - 2:13 Limits and Derivatives of multivariable functions
    - 3:02 Double Integrals
    - 3:44 Triple Integrals and 3D coordinate systems
    - 4:57 Coordinate Transformations and the Jacobian
    - 6:02 Vector Fields, Scalar Fields, and Line Integrals
      - https://youtu.be/5kwz7ajxfyA?si=3eqhsvXLRnXHf1JW
      - [비슷한거ALL OF Calculus 2 in 5 minutes | FuzzyPenguinAMS](https://youtu.be/M9W5Fn0_WAM?si=SJpAtgSQU306xFL1)

- [(part1)Linear Algebra - Full College Course | freeCodeCamp.org](https://youtu.be/JnTa9XtvmfI?si=-oAkPTXieCU78HLA)
- [(part2)Linear Algebra - Full College Course (Part 2)freeCodeCamp.org](https://youtu.be/DJ6YwBN7Ya8?si=fMFLVP6-Xzd5Gy-K)
  - [영상 모아보기) Essence of linear algebra | 3Blue1Brown](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&si=TzIiErHaNEQSHh3V)
- [54hr) College Algebra Full Course | GreenMath.com](https://youtu.be/AuixBGkLjfo?si=92xUoMz7KkoCou8u)

<hr />

- [Calculus 1 - Full College Course | freeCodeCamp.org](https://youtu.be/HfACrKJ_Y2w?si=R7_xKg0-eOJ_JabT)


<hr />

- AI 의 큰 그림으로 개념 잡기
  - AI 데이터 인프라의 부상 (felicis.com)(https://news.hada.io/topic?id=17940)
    - [여기에 그대도 복사함. fork자료](#ai-큰-그림으로-개념-잡기)
  - [대전환 인터뷰:rocket: 'AI 4대 천황' 제프리 힌턴 교수, 10년 후 AI를 전망하다 | 미래기획 대전환 | KBS 20241109 방송 | KBS 다큐](https://youtu.be/SN-BISKo2lE?si=ht4r_aXsw7AWLjCl) 

<hr />

- 머신러닝 기초 원리
  - [AI혁명의 시작 딥러닝 과연 노벨 물리학상은 왜 딥러닝에 주목하였을까? 2024 노벨물리학상해설 1부 [보이저엑스 남세동 대표] | 안될과학 Unrealscience](https://youtu.be/I0UJ5bn0o-I?si=BDR8iK5tSSizRpk-)
  - [알려주지 않은 감정 뉴런이 인공신경망에 생기기 시작했다?! 퍼셉트론의 발전과정과 겨울 그리고 극복! 2024 노벨물리학상해설 2부! [보이저엑스 남세동 대표] | 안될과학 Unrealscience](https://youtu.be/A7PbaeuVhNA?si=2zqCVKKucOf6c-Zi)
  - [갑자기 딥러닝이 주류가 된이유?! 인공지능의 발전이 온 계기와 생성형 AI의 원리 2024 노벨물리학상해설 3부! [보이저엑스 남세동 대표] 안될과학 Unrealscience](https://youtu.be/RjDijmYZftg?si=6hIIcIjf5oXLUf1N)
  - 외국 영상 자동 번역 돌려봐야함
    - [(220816)Hopfield network: How are memories stored in neural networks? /Nobel Prize in Physics 2024/ SoME2 | Layerwise Lectures](https://youtu.be/piF6D6CQxUw?si=PwhSyLSDb5u2O0Kc)


- CNN영상에 주로 사용 / RNN 글씨 문자에 사용 
  - [wiki정의-합성곱 신경망(콘볼루션 신경망, Convolutional neural network, CNN)은 시각적 영상을 분석하는 데 사용되는 다층의 피드-포워드적인 인공신경망의 한 종류이다](https://ko.m.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D)
  - https://ebbnflow.tistory.com/119
    - CNN https://ebbnflow.tistory.com/138
      - CNN은 보통 정보추출, 문장분류, 얼굴인식 등의 분야에서 널리 사용되고 있습니다. 출처: https://ebbnflow.tistory.com/119 [삶은 확률의 구름:티스토리]

- 머신러닝+딥러닝 책 보는 순서-총 정리!!(Book-RoadMap) 
  - https://youtu.be/WHn5My6dN7c
    - 어떤 책을 봐야 하나요? (버전2)
      - https://youtu.be/lp_qGf--gJg

<hr />

- [도쿄대 0.01%만 푸는 정적분 | 교양수학 김희진](https://youtu.be/orvRra1Z2ms?si=tZkaS3aMtNdq8-W-)


<hr />

- [Fast sigmoid algorithm 공식_출처 : stackoverflow.com](https://stackoverflow.com/questions/10732027/fast-sigmoid-algorithm)
  - [sigmoid공식 보기The sigmoid function is defined as & 그래프 그림으로 sigmoid함수 이해하기](#fast-sigmoid-algorithm-공식_출처--stackoverflowcom)
- [What is the role of the bias in neural networks? [closed] 공식_출처 : stackoverflow.com](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)

- Rust언어로 구현한 코드
  - [Rust언어(burn) sigmoid function](https://docs.rs/burn/latest/burn/nn/struct.Sigmoid.html)

- C언어로 구현한 코드
  - [C언어로 구현한 sigmoid function](https://github.com/nayayayay/sigmoid-function)

<hr />

- eBooks
  - [딥러닝 기초를 위한 선형대수학khanacademy.org/math/linear-algebra](https://ko.khanacademy.org/math/linear-algebra)
  - [딥러닝 기초eBook& 행렬기초pytorch-Deep-Learning/ko/](https://atcold.github.io/pytorch-Deep-Learning/ko/)

<hr />

# 행렬 기본 연산 | 선형대수학의 행렬방법, 파트 1 | MATLAB Korea[|🔝|](#link)

- https://youtu.be/ZlId8c6p09o?si=AjSeQd8OgfyVNpnD
  - 교재 github https://github.com/MathWorks-Teaching-Resources/Matrix-Methods-of-Linear-Algebra_ko

- 매트릭스 표기는

```math
rows \times columns
```

- [matrix매트릭스 곱셈 그림으로 이해하기( Visual Understanding: Matrix Multiplication for Dummies | Harvie](https://youtu.be/TwliA2BL_9g?si=eeTHLXPQ5xPaMfA1)
 

<hr />

# Square matrix(정방 행렬)[|🔝|](#link)

```math
 A = \begin{pmatrix}3 & 9 & 3 & 3 \\
5 & 4 & 6 & 8 \\
3 & 9 & 3 & 3 \\
1 & 2 & 3 & 4 \\ \end{pmatrix}
```

# diagonal matrix(대각 행렬)
```math
 A = \begin{pmatrix}3 & 0 & 0 & 0 \\
0 & 4 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 4 \\ \end{pmatrix}
```

# identity matrix(단위 행렬) 
- 대각 요소가 1

```math
 A = \begin{pmatrix}1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\ \end{pmatrix}
```


<hr />

# Machine_Learning_Rust[|🔝|](#link)

# 머신러닝기초
- Machine Learning Course for Beginners | freeCodeCamp.org
  - https://youtu.be/NWONeJKn6kc?si=wmh6EmpSKH1ZIzXH

- Understanding NVIDIA GPU Hardware as a CUDA C Programmer | Episode 2: GPU Compute Architecture | 0Mean1Sigma
  - https://youtu.be/1Goq8Yc3dfo?si=KgztF66DmIwP7qa9
- Awesome Production Machine Learning
  - This repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning 🚀
  - https://github.com/EthicalML/awesome-production-machine-learning 


<hr>

- Rust code들 모음
  - 최신코드 많음 
    - https://github.com/e-tornike/best-of-ml-rust
  - 좋긴 한데 옛날꺼...
    - [This repository accompanies Practical Machine Learning with Rust by Joydeep Bhattacharjee (Apress, 2020).https://github.com/Apress/practical-machine-learning-w-rust](https://github.com/Apress/practical-machine-learning-w-rust)
    - https://github.com/vaaaaanquish/Awesome-Rust-MachineLearning


<hr />

# python vector 이해하기[|🔝|](#link)

```py
plane = NumberPlane()
pi_creature = PiCreature(color=PINK)
plane.prepare_for_nonlinear_transformation()
plane.add(pi_creature)
def homotopy(x,y,z,t):
    norm = np.linalg.norm([x, y])
    tau = interpolate(5, -5,t) + norm/SPACE_WIDTH
    alpha = sigmodi(tau)
    return [x, y+0.5*np.sin(2*np.pi*alpha), z]
play(HomotopyAnimation(homotopy, plane, run_time=3)
```

# Fast sigmoid algorithm 공식_출처 : stackoverflow.com[|🔝|](#link)
- https://stackoverflow.com/questions/10732027/fast-sigmoid-algorithm

```math

S(t) = \frac{1}{1+e^{-t}} 

```

- sigmoid 그래프로 이해하기
  - https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks

<img src="https://i.sstatic.net/ddyfr.png" />

- `S(t) = 1 / (1 + e^(-t))` (where `^` is `pow`)

I found that using the C built-in function `exp()` to calculate the value of `f(x)` is slow. Is there any faster algorithm to calculate the value of `f(x)`?

- 공식을 그래프로 보자 demos랑 다른거
  - demos로 만들어봄 https://www.desmos.com/calculator/zdoveye7qb
  - https://www.wolframalpha.com/input?i=+series+1+%2F+%281+%2B+e+%5E+%28-x%29%29+at+x%3D1

- C언어로
  - https://gist.github.com/astanin/5270668

```c
#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <math.h>

#define M_PI_2        1.57079632679489661923	/* pi/2 */
#define M_PI_2_INV    (1.0/M_PI_2)
#define M_2_SQRTPI    1.12837916709551257390    /* 2/sqrt(pi) */
#define ERF_COEF      (1.0/M_2_SQRTPI)

const int SIZE=100;
const int CYCLES=10000000;

double benchmark(const char* name, double (*fun)(double)) {
	clock_t start, stop;
	double xs[SIZE];
	double t_ns;

	for (int i=0; i<SIZE; i++) {
		xs[i] = rand();
	}

	start = clock();
	for (int repeat=0; repeat<CYCLES; repeat++) {
		for (int i=0; i<SIZE; i++) {
			(*fun)(xs[i]);
		}
	}
	stop = clock();
	t_ns = (stop-start)*1.0e9/CLOCKS_PER_SEC/CYCLES/SIZE;
        printf("%-17s %6.1f ns\n", name, t_ns);
	return t_ns;
}

double with_atan(double x) {
	/* normalized atan */
	return M_PI_2_INV*atan(M_PI_2*x);
}

double with_exp(double x) {
	return 1.0/(1.0 + exp(-x));
}

double with_sqrt(double x) {
	return 1.0/sqrt(1.0 + x*x);
}

double with_erf(double x) {
	return erf(ERF_COEF*x);
}

double with_fabs(double x) {
	return x/(1.0 + fabs(x));
}

int main(int argc, char **argv) {
	benchmark("atan(pi*x/2)*2/pi", with_atan);
	benchmark("atan(x)", atan);
	benchmark("1/(1+exp(-x))", with_exp);
	benchmark("1/sqrt(1+x^2)", with_sqrt);
	benchmark("erf(sqrt(pi)*x/2)", with_erf);
	benchmark("tanh(x)", tanh);
	benchmark("x/(1+|x|)", with_fabs);
}
```

- On my Core i5-3317U with GCC 4.7.2:
```bash
% gcc -Wall -O2 -lm -o sigmoid-bench{,.c} -std=c99 && ./sigmoid-bench
atan(pi*x/2)*2/pi   24.1 ns
atan(x)             23.0 ns
1/(1+exp(-x))       20.4 ns
1/sqrt(1+x^2)       13.4 ns
erf(sqrt(pi)*x/2)    6.7 ns
tanh(x)              5.5 ns
x/(1+|x|)            5.5 ns
```

# AI 큰 그림으로 개념 잡기[|🔝|](#link)

- 241125기준자료
  - https://www.felicis.com/insight/ai-data-infrastructure

<div id="msg"></div><div class="topic"><div class="vote"><span id="vote17940"><a class="upvote" href="javascript:vote(17940, &quot;up&quot;);"><span>▲</span></a></span></div><div class="topictitle link"><a href="https://www.felicis.com/insight/ai-data-infrastructure" class="bold ud"><h1>AI 데이터 인프라의 부상</h1></a> <span class="topicurl">(felicis.com)</span></div><div class="topicinfo"><span id="tp17940">12</span>P by <a href="/user?id=xguru">xguru</a> 241125<span id="unvote17940"></span><span id="fav17940"> | <a href="javascript:fav(17940, &quot;favorite&quot;)">favorite</a></span> | <a href="topic?id=17940">댓글과 토론</a> </div><div class="topic_contents"><div><span id="topic_contents"><blockquote>
<p>"우리는 현재 새로운 산업 혁명의 시작에 있음. 전기 생산 대신 인공지능을 생성하는.. [오픈소스]는 모든 기업이 인공지능 기업이 될 수 있게 함" - 젠슨 황</p>
</blockquote>
<ul>
<li>문서에서 정보를 추출하는 것은 새로운 개념은 아님. 하지만 생성형AI(GenAI)는 대량의 고품질 데이터를 필요로 함</li>
<li>훈련과 추론 모두에 데이터가 중요하며 데이터 규모뿐만 아니라 텍스트, 테이블 데이터에서 비디오, 이미지, 오디오로 확장됨</li>
<li>위성 이미지, 로봇 센서 데이터 등 공간 데이터의 증가도 관찰됨</li>
<li>데이터 계층에서 AI로 인해 가장 즉각적으로 재창조될 수 있는 새로운 영역은 무엇일까?
<ul>
<li>비정형 데이터 추출과 파이프라인, 검색 증강 생성 (Retrieval-Augmented Generation, RAG), 데이터 큐레이션, 데이터 스토리지 , - 인공지능 메모리</li>
</ul>
</li>
<li>이 글의 목적은 AI 데이터 인프라 환경을 분석하고, 최신 트렌드를 공유하고, 가장 유망한 혁신 영역에 대해 이야기 하는 것</li>
</ul>
<h2>AI 데이터 인프라 현황</h2>
<ul>
<li>AI 데이터 가치 사슬에서 데이터 흐름을 간단히 시각화하며, 데이터 학습 및 추론 과정의 흐름을 설명하고자 함</li>
<li>데이터 인프라의 가치 사슬을 여섯 개 주요 영역으로 분류
<ul>
<li>데이터 소스 (Sources)</li>
<li>데이터 수집 및 변환 (Ingestion &amp; Transformation)</li>
<li>저장 (Storage)</li>
<li>훈련 (Training)</li>
<li>추론 (Inference)</li>
<li>데이터 서비스 (Data Services)</li>
</ul>
</li>
</ul>
<h3>데이터 소스</h3>
<ul>
<li>
<strong>앱 데이터</strong>: Salesforce, ServiceNow 등에서 추출</li>
<li>
<strong>실시간 데이터</strong>: 센서, 제조, 의료 데이터</li>
<li>
<strong>OLTP 데이터베이스</strong>: Oracle, MongoDB와 같은 트랜잭션 데이터</li>
<li>
<strong>합성 데이터</strong>: 현실 세계에서 수집하지 않은 인공 생성 데이터 (e.g., Mostly AI, Datagen, Tonic)
<ul>
<li>비용 효율적이고 데이터 준수 측면에서 유리함</li>
<li>그러나, 통계적 이상치 데이터 표현이 부족해 모델 성능 최적화에 한계 있음</li>
</ul>
</li>
<li>
<strong>웹 데이터</strong>: 웹 스크래핑을 통해 공용 데이터를 수집 (e.g., Browse AI, Apify)
<ul>
<li>대규모 데이터 모델 훈련에 필수적이나, 공개 데이터가 고갈될 가능성 있음 (2026~2032년 예상)</li>
</ul>
</li>
</ul>
<h3>데이터 수집 및 변환</h3>
<ul>
<li>데이터 파이프라인은 데이터의 출발지에서 목적지로 데이터를 전송하고 분석 가능한 상태로 변환하는 과정
<ul>
<li>
<strong>ETL/ELT</strong>: 전통적인 방식 (배치 처리, 스트리밍 처리)</li>
<li>
<strong>피쳐 엔지니어링/파이프라인</strong>: ML에서는 주로 테이블 데이터 처리</li>
<li>
<strong>비정형 데이터 파이프라인</strong>: 데이터 추출, 변환, 저장 과정을 통합하여 비정형 데이터를 정리 및 저장</li>
</ul>
</li>
<li>
<strong>파이프라인 유형</strong>
<ul>
<li>
<strong>배치 처리</strong>: 특정 시간 간격으로 데이터를 추출 및 적재</li>
<li>
<strong>스트리밍 처리</strong>: 데이터를 실시간으로 로드 (Kafka, Flink등)</li>
</ul>
</li>
<li>
<strong>도구 및 프레임워크</strong>
<ul>
<li>스트리밍 (Kafka, Confluent), 처리 엔진 (Databricks, Flink), 오케스트레이션 도구 (Astronomer, Dagster, Airflow, Prefect 등)</li>
<li>라벨링 도구: LabelBox, Scale AI 등 (테스트 데이터 라벨링 중요)
<ul>
<li>배치: ETL(Airbyte, Fivetran), 트랜스폼(dbt,coalesce)</li>
<li>비정형 데이터 처리: Datavolo, Unstructured, LlamaIndex 등</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>데이터 저장</h3>
<ul>
<li>
<strong>전통적 접근</strong>: 데이터 웨어하우스에 저장</li>
<li>
<strong>AI 활용 데이터</strong>:
<ul>
<li>데이터 레이크와 레이크하우스 구조 활용</li>
<li>벡터 데이터베이스를 통한 데이터 임베딩 저장</li>
</ul>
</li>
<li>
<strong>주요 도구</strong>:
<ul>
<li>데이터 레이크 : Databricks, Onehouse, Tabular, Amazon S3, GCS 등
<ul>
<li>벡터 DB: Pinecone, Chroma, Milvus, Weaviete  등</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>모델 훈련</h3>
<ul>
<li>
<strong>학습 방식</strong>:
<ul>
<li>지도 학습, 비지도 학습, 강화 학습</li>
</ul>
</li>
<li>
<strong>대규모 언어 모델(LLM) 학습 과정</strong>:
<ul>
<li>사전 학습: 비지도 학습으로 데이터의 패턴 인식</li>
<li>지도 학습: 성능 최적화</li>
<li>강화 학습(RLHF): 인간 피드백을 통한 성능 향상</li>
</ul>
</li>
<li>
<strong>검증 및 평가</strong>:
<ul>
<li>정확도, 정밀도, 손실 최소화 등 모델의 적합성 평가</li>
</ul>
</li>
<li>
<strong>최종 단계</strong>:
<ul>
<li>보안 테스트, 거버넌스, 컴플라이언스 확인</li>
</ul>
</li>
<li>
<strong>주요 도구</strong>:
<ul>
<li>트레이닝: TensorFlow, Modular
<ul>
<li>Evaluation: neptune.ai, Weights &amp; Biases</li>
<li>MLOps: Databricks, H2O.ai, DataRobot, Dataiku, DOMINO</li>
<li>Model: OpenAI, Cohere, Mistral AI, Runway</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>모델 추론</h3>
<ul>
<li>
<strong>과정</strong>:
<ul>
<li>프롬프트 입력 → 토큰화/벡터화 → 데이터 처리 → 출력 생성</li>
</ul>
</li>
<li>
<strong>맞춤화</strong>:
<ul>
<li>벡터 데이터베이스와 LLM 연동</li>
<li>사용자의 컨텍스트를 반영한 고유한 결과 생성</li>
</ul>
</li>
<li>
<strong>필수 고려사항</strong>:
<ul>
<li>데이터 보안, 모델 품질, 컴플라이언스</li>
</ul>
</li>
<li>
<strong>주요 도구</strong>:
<ul>
<li>Tooling: ANON, E2B</li>
<li>Memory: MemGPT, cognee.ai</li>
<li>RAG Framework: LangCHain, LlamaIndex, contextual.ai, databricks</li>
<li>Agent/App: ChatGPT, Claude, character.ai, Decagon, NormAi</li>
</ul>
</li>
</ul>
<h3>데이터 서비스</h3>
<ul>
<li>
<strong>범주</strong>:
<ul>
<li>데이터 보안: 접근 제어, 데이터 유출 방지 (Rubrik, eureka, imperva, sentra, Dig, Cyera, Varonis, BigID)</li>
<li>데이터 가시성: 데이터 파이프라인의 품질 및 성능 모니터링 (Anomalo, datologyai, OBSERVE, MonteCarlo, Cleanlab, Scale AI, onum, metaplane)</li>
<li>데이터 카탈로그: 메타데이터 중앙화, 데이터 자산 조직화 (atlan, Alation, Collibra, Informatica, Acryl Data, CastorDoc, select star, data.world)</li>
</ul>
</li>
<li>
<strong>결론</strong>:
<ul>
<li>데이터가 잘 조직화될수록 보안, 가시성, 관리가 효율적임</li>
</ul>
</li>
</ul>
<hr>
<h2>[AI로 인한 데이터 재구성]</h2>
<p>AI로 인해 데이터 인프라의 다음 영역에서 혁신이 관찰됨:</p>
<h3>1. AI 에이전트 및 애플리케이션을 위한 비정형 데이터 파이프라인</h3>
<ul>
<li>
<strong>비정형 데이터 파이프라인의 부상</strong>:
<ul>
<li>대화형 AI 및 에이전트 애플리케이션에 내부 비정형 데이터를 활용하려는 수요 증가</li>
<li>비정형 데이터 파이프라인은 전통적 데이터 파이프라인과 유사한 과정 포함: 데이터 추출, 변환, 인덱싱, 저장</li>
</ul>
</li>
<li>
<strong>주요 데이터 소스</strong>:
<ul>
<li>PDF 텍스트, 지식 베이스, 이미지 등</li>
<li>주로 대화형 AI 활용 사례를 지원하는 데이터</li>
</ul>
</li>
<li>
<strong>차별화 요소</strong>:
<ul>
<li>변환 단계에서 기존 파이프라인과의 차이 발생:
<ul>
<li>데이터 청킹(chunking): 데이터를 작은 단위로 나누기</li>
<li>메타데이터 추출: 인덱싱을 위해 필요한 데이터 생성</li>
<li>임베딩: 각 데이터 청크를 벡터 형태로 변환해 저장</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>성공 요소</strong>:
<ul>
<li>청킹 전략과 임베딩 모델의 선택은 데이터 검색 정확성에 중요한 영향을 미침</li>
<li>도메인 특화 임베딩 모델의 등장: 예를 들어 코드, 법률 콘텐츠에 특화된 모델</li>
</ul>
</li>
<li>
<strong>벡터 호환 데이터베이스 활용</strong>:
<ul>
<li>비정형 데이터를 저장하고 질의 가능한 형식으로 변환</li>
<li>RAG(Retrieval-Augmented Generation) 및 에이전트를 통해 LLM 개인화 가능</li>
</ul>
</li>
<li>
<strong>주요 관찰</strong>
<ul>
<li>팀들은 다양한 청킹 전략을 시도하고 있음</li>
<li>도메인별 특화된 임베딩 모델이 점차 증가하며 정확도와 성능 개선에 기여</li>
<li>기업들은 데이터를 쉽게 질의할 수 있는 형식으로 변환하는 도구를 찾고 있음</li>
</ul>
</li>
</ul>
<h3>2. Retrieval-Augmented Generation (RAG)</h3>
<ul>
<li>
<strong>RAG 개요</strong>:
<ul>
<li>RAG는 LLM 애플리케이션의 효율성을 개선하기 위해 사용자 정의 데이터를 활용하는 아키텍처적 워크플로</li>
<li>
<strong>작동 방식</strong>:
<ul>
<li>데이터를 로드하고 질의 처리를 위해 "인덱싱"</li>
<li>질의는 인덱스를 기반으로 가장 관련성 높은 데이터를 필터링</li>
<li>필터링된 컨텍스트와 질의가 LLM과 프롬프트로 전달되어 응답 생성</li>
</ul>
</li>
<li>데이터를 제품 경험의 일부로 활성화 가능</li>
</ul>
</li>
<li>
<strong>RAG의 주요 장점</strong>:
<ul>
<li>
<strong>업데이트된 정보 제공</strong>:
<ul>
<li>LLM은 사전 학습 데이터에 제한이 있어 오래되거나 부정확한 응답 가능성 존재</li>
<li>RAG는 외부 정보 소스에 접근해 최신 응답 제공</li>
</ul>
</li>
<li>
<strong>사실성 강화</strong>:
<ul>
<li>LLM이 정확한 정보를 제공하지 못하는 문제를 RAG가 보완</li>
<li>선별된 지식 베이스를 활용해 신뢰도 높은 정보를 제공</li>
</ul>
</li>
<li>
<strong>출처 제공</strong>:
<ul>
<li>LLM의 응답에 인용 및 주석 추가 가능</li>
<li>사용자 신뢰도 향상</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>3. 학습 및 추론 성능 향상을 위한 데이터 큐레이션</h3>
<ul>
<li>
<strong>데이터 큐레이션</strong>: 최적의 학습 및 추론 성능을 위해 데이터셋을 필터링하고 구성하는 과정
<ul>
<li>주요 작업:
<ul>
<li>텍스트 분류</li>
<li>NSFW 필터 적용</li>
<li>데이터 중복 제거</li>
<li>배치 크기 최적화</li>
<li>성능 기반 소스 최적화</li>
<li>합성 데이터를 통한 데이터 증강</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Meta Llama-3 발표에서의 인사이트</strong>:
<ul>
<li>
<strong>학습 데이터 큐레이션</strong>:
<ul>
<li>"최고의 언어 모델을 학습시키기 위해 고품질 대규모 데이터셋의 큐레이션이 중요"</li>
<li>Meta는 다음과 같은 데이터 필터링 파이프라인 개발:
<ul>
<li>휴리스틱 필터</li>
<li>NSFW 필터</li>
<li>의미적 중복 제거</li>
<li>데이터 품질 예측 텍스트 분류기</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>미세 조정 데이터 큐레이션</strong>:
<ul>
<li>"모델 품질의 가장 큰 개선은 데이터를 신중히 큐레이션하고, 인간 주석가의 주석을 다수의 품질 보증 단계를 통해 검토함으로써 달성됨"</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>데이터 큐레이션의 효과</strong>:
<ul>
<li>Meta AI 연구팀에 따르면:
<ul>
<li>큐레이션은 학습 시간을 최대 20% 단축</li>
<li>다운스트림 정확도 개선</li>
<li>인터넷 데이터 고갈 상황에서도 모델 성능 개선 경로 제공</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>향후 방향</strong>:
<ul>
<li>모델 학습 및 미세 조정을 위해 자동화된 고품질 데이터 필터, 중복 제거, 분류기가 중요</li>
<li>Datology AI와 같은 기업이 이를 실현하기 위해 노력 중</li>
</ul>
</li>
</ul>
<h3>4. AI를 위한 데이터 저장</h3>
<ul>
<li>AI 데이터를 저장하는 방식에는 세 가지 주요 트렌드가 있음:
<ul>
<li>벡터 데이터베이스</li>
<li>데이터 레이크의 부상</li>
<li>레이크하우스에 대한 투자 증가</li>
</ul>
</li>
<li>
<strong>벡터 데이터베이스의 중요성</strong>:
<ul>
<li>벡터 데이터베이스는 AI 붐의 핵심 기술 중 하나로 주목받음</li>
<li>데이터 임베딩(숫자 표현) 저장에 적합:
<ul>
<li>비정형 데이터(이미지, 오디오, 비디오 등)를 수치로 변환하여 저장</li>
<li>의미적 검색(예: "dog" 검색 시 "wolf" 또는 "puppy" 반환) 지원</li>
</ul>
</li>
<li>
<strong>벡터 데이터베이스의 형태</strong>:
<ul>
<li>
<strong>네이티브 벡터 데이터베이스</strong>: 벡터 저장 전용으로 설계됨</li>
<li>
<strong>기존 데이터베이스 확장형</strong>: 기존 데이터베이스에 벡터 지원 기능 추가</li>
</ul>
</li>
<li>
<strong>활용 사례</strong>: LLM 개인화
<ul>
<li>기업의 커스텀 데이터를 벡터 임베딩으로 저장하고 검색 가능</li>
<li>AI 에이전트가 이 구조를 활용해 맞춤형 경험 제공</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>데이터 레이크 및 레이크하우스</strong>
<ul>
<li>
<strong>데이터 레이크의 부상</strong>:
<ul>
<li>대부분의 기업이 대규모 데이터를 데이터 레이크에 저장</li>
<li>커스텀 AI 개발을 위해 데이터 레이크 활용 필수</li>
</ul>
</li>
<li>
<strong>레이크하우스 아키텍처</strong>:
<ul>
<li>데이터 레이크를 효과적으로 관리하고 질의할 수 있는 아키텍처 제공</li>
<li>
<strong>오픈 테이블 포맷</strong>으로 데이터 구성:
<ul>
<li>Iceberg, Delta Lake, Hudi 등 활용</li>
</ul>
</li>
<li>데이터 조직화 및 쿼리 성능 향상</li>
</ul>
</li>
<li>
<strong>Databricks의 역할</strong>:
<ul>
<li>Databricks는 Tabular를 인수하여 Delta Lake와 Iceberg의 개발팀 통합</li>
<li>경쟁사의 진입을 어렵게 하며 레이크하우스 기술 발전을 선도</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>5. AI 메모리</h3>
<ul>
<li>
<strong>AI 메모리의 부상</strong>:
<ul>
<li>ChatGPT의 메모리 기능 발표 이후 AI 메모리가 주요 논의 주제로 떠오름</li>
<li>표준 AI 시스템은 강력한 에피소드 메모리 및 상호작용 간 연속성이 부족:
<ul>
<li>현재 시스템은 일종의 단기 기억 상실 상태에 있음</li>
<li>복잡한 순차적 추론과 다중 에이전트 시스템에서의 지식 공유에 제약</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>다중 에이전트 시스템에서의 메모리</strong>
<ul>
<li>다중 에이전트 시스템으로 발전함에 따라 에이전트 간 메모리 관리 시스템이 필요</li>
<li>
<strong>기능 요구사항</strong>:
<ul>
<li>에이전트 별로 기억 저장 및 세션 간 접근 지원</li>
<li>접근 및 개인정보 보호 통제 포함</li>
<li>에이전트 간 메모리 풀링:
<ul>
<li>한 에이전트가 다른 에이전트의 경험을 활용 가능</li>
<li>의사결정 능력 향상</li>
</ul>
</li>
</ul>
</li>
<li>계층적 메모리 필요:
<ul>
<li>접근 빈도, 중요도, 비용에 따라 메모리를 계층적으로 저장</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>MemGPT: AI 메모리 관리의 선도 프레임워크</strong>
<ul>
<li>MemGPT의 비전: LLM이 차세대 운영 체제(OS)의 진화를 이끌 것이라는 목표</li>
<li>
<strong>아키텍처 개요</strong>:
<ul>
<li>
<strong>메모리 유형</strong>:
<ul>
<li>
<strong>주요 컨텍스트 메모리</strong>: 주 메모리(RAM)와 유사</li>
<li>
<strong>외부 컨텍스트 메모리</strong>: 디스크 메모리/디스크 스토리지와 유사</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>AI 메모리의 중요성</strong>
<ul>
<li>개인화, 학습, 반성(reflection)을 지원하며 AI 애플리케이션 발전에 필수적</li>
<li>에이전트 간 협력과 기억 공유를 통해 복잡한 작업 해결 능력 향상</li>
</ul>
</li>
</ul>
<h3>AI 워크로드의 기회</h3>
<ul>
<li>
<strong>AI 워크로드와 데이터 인프라</strong>:
<ul>
<li>GenAI의 부상으로 데이터 인프라의 모든 측면이 변화한 것은 아니지만, 다음과 같은 기술의 등장은 매우 흥미로운 발전:
<ul>
<li>비정형 데이터 추출 및 파이프라이닝</li>
<li>Retrieval-Augmented Generation (RAG)</li>
<li>데이터 큐레이션</li>
<li>데이터 저장</li>
<li>AI 메모리</li>
</ul>
</li>
</ul>
</li>
<li>Felicis의 투자 전략
<ul>
<li>
<strong>AI와 데이터 인프라의 미래에 집중</strong>:
<ul>
<li>데이터 및 인프라 계층 관련 스타트업에 투자</li>
<li>주요 투자 사례:
<ul>
<li>
<strong>Datology</strong>: 데이터 큐레이션<a href="https://www.datologyai.com/">DatologyAI홈페이지</a></li>
<li>
<strong>Metaplane</strong>: 데이터 관찰 가능성(data observability)<a href="https://www.metaplane.dev/">metaplane.dev홈페이지</a>__비상장회사 정보<a href="https://www.cbinsights.com/company/metaplane/financials">metaplane.dev비상장회사 정보 보기</a></li>
<li>
<strong>MotherDuck</strong>: 서버리스 데이터 웨어하우스<a href="https://forgeglobal.com/motherduck_ipo/">MotherDock 비상장회사 정보 보기</a></li>
<li>
<strong>Weights &amp; Biases</strong>: 실험 추적 도구<a href="https://forgeglobal.com/weights-biases_stock/">Weights & Biases 비상장회사 정보 보기</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>AI 시장의 성장 가능성
<ul>
<li>
<strong>확장 가능성</strong>:
<ul>
<li>AI 시장은 채팅봇에서 다중 에이전트 워크플로까지 광범위하게 확장 중</li>
<li>현재는 시작 단계에 불과하며 앞으로 더 많은 발전 가능성 존재</li>
</ul>
</li>
<li>
<strong>데이터 솔루션의 중요성</strong>:
<ul>
<li>성공적인 AI 애플리케이션을 위해 데이터 솔루션이 핵심</li>
<li>AI 워크로드를 지원하는 대규모 데이터 비즈니스가 구축될 전망</li>
</ul>
</li>
</ul>
</li>
</ul>
</span></div></div></div>

<hr />

<br />

<hr />
            
